{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import googlemaps as gm\n",
    "from geopy.geocoders import Nominatim\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section, I will be taking the Menu table that Vidya has used OpenRefine to clean and cluster places and locations. We will take this one step further and harness the Google Maps places API and the Geopy API to go from place names, to latitude/longitude coordinates, and finally to regions (here as state + country or just country where state does not exist). If the places or locations are unknown, or the Google Maps API is unable to find a place, then the final entry will be \"UNKNOWN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = gm.Client(os.getenv('GMAPS_API'))\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_df = pd.read_csv(\"Menu-edited-meals-place-location-year.csv\")\n",
    "menu_edit = menu_df.copy()\n",
    "menu_edit['loc_place'] = menu_edit['location_edited'] +', '+menu_edit['place_edited']\n",
    "locations = menu_edit[(menu_edit.place_edited.str.lower()!='unknown')&(menu_edit.location_edited.str.lower()!='unknown')].loc_place.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_region() will take a location string (concatenated as location, place above) and convert it to a region where possible (returning None if not). make_region_map() will add the result of get_region() to a dictionary if it is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(loc_string):\n",
    "    results = gmaps.places(loc_string)\n",
    "    if not results['results']:\n",
    "        return None\n",
    "    lat_long = results['results'][0]['geometry']['location']\n",
    "    location = geolocator.reverse(str(lat_long['lat'])+\",\"+str(lat_long['lng']))\n",
    "    addr = location.raw['address']\n",
    "    if 'state' not in addr:\n",
    "        if 'country' not in addr:\n",
    "            return None\n",
    "        return addr['country']\n",
    "    return addr['state'] +', '+ addr['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_region_map(locations):\n",
    "    mapping = {}\n",
    "    for loc in locations:\n",
    "        region = get_region(loc)\n",
    "        if region:\n",
    "            mapping[loc] = region\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have commented out the actual call to make_region_map since it takes a while to run and calls the google maps api. Instead, below I save the initial run to a json file and upload that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings = make_region_map(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mappings.json', 'w') as f:\n",
    "#     json.dump(mappings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('mappings.json')\n",
    "mappings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply_region() will run over every entry in the Menu table and either apply the corresponding region mapping from the mapping dictionary, or \"UNKNOWN\" if no entry exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_region(df):\n",
    "    loc_place = df['loc_place']\n",
    "    if loc_place in mappings:\n",
    "        df['region'] = mappings[loc_place]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_edit['region'] = 'UNKNOWN'\n",
    "menu_edit = menu_edit.apply(apply_region, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Menus with Unknown Location: 12746 Number of Menus: 17547\n"
     ]
    }
   ],
   "source": [
    "print('Number of Menus with Unknown Location:',menu_edit.region.value_counts()['UNKNOWN'], \"Number of Menus:\", menu_edit.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Dates for Dishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to more easily work with the year column in the Menu table to correct and fill the first and last appeared dates in the Dishes table, I convert it to a datetime format where it exists, or a NaT format where year is unknown (or out of bounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_edit['year_edited'] = menu_edit['year']\n",
    "menu_edit.loc[menu_edit.year_edited.str.lower()=='unknown', 'year_edited'] = '0'\n",
    "menu_edit['year_edited'] = menu_edit['year_edited'].astype(int)\n",
    "menu_edit.loc[(menu_edit.year_edited<1848)|(menu_edit.year_edited>2021), 'year_edited'] = np.nan\n",
    "menu_edit['year_edited_dt'] = pd.to_datetime(menu_edit.year_edited, format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Menus missing years: 591\n",
      "Number of Menus: 17547\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Menus missing years:\", menu_edit['year_edited_dt'].isnull().sum())\n",
    "print(\"Number of Menus:\", menu_edit.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "menupage_df = pd.read_csv('MenuPage.csv')\n",
    "mp_edit = menupage_df.copy()\n",
    "menuitem_df = pd.read_csv('MenuItem.csv')\n",
    "mi_edit = menuitem_df.copy()\n",
    "dishes_df = pd.read_csv('NYPL-dishes-v2.csv')\n",
    "dishes_edit = dishes_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply_date_fix() assign an NaT value to any incorrect dates in the Dishes table (either entries equal to '0', '1' or in the future and unable to be converted to date time). Otherwise, first and last appeared dates are converted to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_date_fix(df):\n",
    "    first_appeared = df['first_appeared']\n",
    "    last_appeared = df['last_appeared']\n",
    "    if first_appeared == '0' or first_appeared == '1':\n",
    "        df['first_appeared_unknown'] = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            df['first_appeared_unknown'] = pd.to_datetime(first_appeared)\n",
    "        except:\n",
    "            df['first_appeared_unknown'] = np.nan\n",
    "        \n",
    "    if last_appeared == '0' or last_appeared == '1':\n",
    "        df['last_appeared_unknown'] = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            df['last_appeared_unknown'] = pd.to_datetime(last_appeared)\n",
    "        except:\n",
    "            df['last_appeared_unknown'] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dishes_edit['first_appeared_unknown'] = dishes_edit['first_appeared']\n",
    "dishes_edit['last_appeared_unknown'] = dishes_edit['last_appeared']\n",
    "dishes_edit = dishes_edit.apply(apply_date_fix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dishes with invalid first_appeared date: 47490\n",
      "Number of dishes with invalid last_appeared date: 47483\n",
      "Total Number of Dishes: 343698\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of dishes with invalid first_appeared date:\",dishes_edit['first_appeared_unknown'].isnull().sum())\n",
    "print(\"Number of dishes with invalid last_appeared date:\",dishes_edit['last_appeared_unknown'].isnull().sum())\n",
    "print(\"Total Number of Dishes:\", dishes_edit.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fill entries in the Dishes table that have invalid first and/or last appeared dates with the correct ones, I first need to merge the Menu table, Menu Item table, Menu Pages table and Dishes table all together. Then, for each remaining Dish entry ID, I get the minimum and maximum non-Null years from the Menus it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "master= mi_edit.merge(mp_edit.merge(menu_edit.rename({'id':'menu_id'}, axis='columns'), on='menu_id')\\\n",
    "                             .rename({'id':'menu_page_id'},axis='columns'), on='menu_page_id')\\\n",
    "                .merge(dishes_edit.rename({'id':'dish_id'},axis='columns'), on='dish_id').rename({'id':'menu_item_id'},axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-7b265f959d7f>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  menu_dish_dt['dish_id'] = menu_dish_dt.dish_id.astype(int)\n"
     ]
    }
   ],
   "source": [
    "menu_dish_dt = master[['menu_id', 'dish_id', 'year_edited_dt', 'first_appeared_unknown', 'last_appeared_unknown']]\n",
    "menu_dish_dt['dish_id'] = menu_dish_dt.dish_id.astype(int)\n",
    "first_corrections = menu_dish_dt[~menu_dish_dt.year_edited_dt.isnull()].groupby('dish_id').year_edited_dt.min().to_dict()\n",
    "last_corrections = menu_dish_dt[~menu_dish_dt.year_edited_dt.isnull()].groupby('dish_id').year_edited_dt.max().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I apply the date corrections where I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_corrected_dt(df):\n",
    "    if df['id'] in first_corrections:\n",
    "        df['first_appeared_corrected'] = pd.to_datetime(first_corrections[df['id']],utc=True)\n",
    "    if df['id'] in last_corrections:\n",
    "        df['last_appeared_corrected'] = pd.to_datetime(last_corrections[df['id']],utc=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dishes_edit['first_appeared_corrected'] = dishes_edit['first_appeared_unknown']\n",
    "dishes_edit['last_appeared_corrected'] = dishes_edit['last_appeared_unknown']\n",
    "dishes_edit = dishes_edit.apply(apply_corrected_dt, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dishes with corrected first_appeared: 49079\n",
      "Number of dishes with corrected last_appeared: 49020\n",
      "Number of Nulls corrected (first_appeared): 11381\n",
      "Number of Nulls corrected (last_appeared): 11375\n",
      "Total Dishes: 343698\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of dishes with corrected first_appeared:\",(pd.DatetimeIndex(dishes_edit.first_appeared_corrected).year!=pd.DatetimeIndex(dishes_edit.first_appeared_unknown).year).sum())\n",
    "print(\"Number of dishes with corrected last_appeared:\",(pd.DatetimeIndex(dishes_edit.last_appeared_corrected).year!=pd.DatetimeIndex(dishes_edit.last_appeared_unknown).year).sum())\n",
    "print(\"Number of Nulls corrected (first_appeared):\", dishes_edit.first_appeared_unknown.isnull().sum() - dishes_edit.first_appeared_corrected.isnull().sum())\n",
    "print(\"Number of Nulls corrected (last_appeared):\", dishes_edit.last_appeared_unknown.isnull().sum() - dishes_edit.last_appeared_corrected.isnull().sum())\n",
    "print(\"Total Dishes:\", dishes_edit.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IVC corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main consideration here outside of what has been done so far is that it seems that there are dishes, menu items, menu pages, and menus that are only accounted for in their respective tables, but are not actually linked to each other. Because pandas by default performs inner joins, we can accomplish this IVC fairly easily by simply removing items in the dishes, menu items, menu pages and menu tables that do not have corresponding entries in the master merge table created above. We will also make sure each table only has one ID per entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Menu Page entries in original: 66937\n",
      "Number of Menu Page entries in final: 26443\n",
      "One ID per entry? True\n"
     ]
    }
   ],
   "source": [
    "menupage_ids = master.menu_page_id.astype(int).unique()\n",
    "mp_edit = mp_edit[mp_edit.id.isin(menupage_ids)]\n",
    "print(\"Number of Menu Page entries in original:\", menupage_df.id.unique().shape[0])\n",
    "print(\"Number of Menu Page entries in final:\", mp_edit.id.unique().shape[0])\n",
    "print(\"One ID per entry?\", mp_edit.shape[0] == mp_edit.id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Menu Item entries in original: 1334784\n",
      "Number of Menu Item entries in final: 1029756\n",
      "One ID per entry? True\n"
     ]
    }
   ],
   "source": [
    "menuitem_ids = master.menu_item_id.astype(int).unique()\n",
    "mi_edit = mi_edit[mi_edit.id.isin(menuitem_ids)]\n",
    "print(\"Number of Menu Item entries in original:\", menuitem_df.id.unique().shape[0])\n",
    "print(\"Number of Menu Item entries in final:\", mi_edit.id.unique().shape[0])\n",
    "print(\"One ID per entry?\", mi_edit.shape[0] == mi_edit.id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Menu entries in original: 17547\n",
      "Number of Menu entries in final: 17504\n",
      "One ID per entry? True\n"
     ]
    }
   ],
   "source": [
    "menu_ids = master.menu_id.astype(int).unique()\n",
    "menu_edit = menu_edit[menu_edit.id.isin(menu_ids)]\n",
    "print(\"Number of Menu entries in original:\", menu_df.id.unique().shape[0])\n",
    "print(\"Number of Menu entries in final:\", menu_edit.id.unique().shape[0])\n",
    "print(\"One ID per entry?\", menu_edit.shape[0] == menu_edit.id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Menu entries in original: 343698\n",
      "Number of Menu entries in final: 331536\n",
      "One ID per entry? True\n"
     ]
    }
   ],
   "source": [
    "dish_ids = master.dish_id.astype(int).unique()\n",
    "dishes_edit = dishes_edit[dishes_edit.id.isin(dish_ids)]\n",
    "print(\"Number of Menu entries in original:\", dishes_df.id.unique().shape[0])\n",
    "print(\"Number of Menu entries in final:\", dishes_edit.id.unique().shape[0])\n",
    "print(\"One ID per entry?\", dishes_edit.shape[0] == dishes_edit.id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_edit.to_csv('MenuPage_cleaned.csv', index=False)\n",
    "mi_edit.to_csv('MenuItem_cleaned.csv', index=False)\n",
    "menu_edit.to_csv('Menu_cleaned.csv', index=False)\n",
    "dishes_edit.to_csv('Dish_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
